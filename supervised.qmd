# Supervised Learning

## Introduction
Supervised learning uses labeled datasets to train algorithms that to classify
data or predict outcomes accurately. As input data is fed into the model, it
adjusts its weights until the model has been fitted appropriately, which occurs
as part of the cross validation process.

In contrast, unsupervised learning uses unlabeled data to discover patterns that
help solve for clustering or association problems. This is particularly useful
when subject matter experts are unsure of common properties within a data set.


## Classification vs Regression

+ Classificaiton: outcome variable is categorical
+ Regression: outcome variable is continuous
+ Both problems can have many covariates (predictors/features)

### Regression metrics

+ Mean squared error (MSE)
+ Mean absolute error (MAE)

### Classification metrics

#### Confusion matrix

<https://en.wikipedia.org/wiki/Confusion_matrix>

Four entries in the confusion matrix:

+ TP: number of true positives
+ FN: number of false negatives
+ FP: number of false positives
+ TN: number of true negatives

Four rates from the confusion matrix with actual (row) margins:

+ TPR: TP / (TP + FN). Also known as sensitivity.
+ FNR: TN / (TP + FN). Also known as miss rate.
+ FPR: FP / (FP + TN). Also known as false alarm, fall-out.
+ TNR: TN / (FP + TN). Also known as specificity.

Note that TPR and FPR do not add up to one. Neither do FNR and FPR.

Four rates from the confusion matrix with predicted (column) margins:

+ PPV: TP / (TP + FP). Also known as precision.
+ FDR: FP / (TP + FP).
+ FOR: FN / (FN + TN).
+ NPV: TN / (FN + TN).


#### Measure of classification performance

Measures for a given confusion matrix:

+ Accuracy: (TP + TN) / (P + N). The proportion of all corrected
  predictions. Not good for highly imbalanced data.
+ Recall (sensitivity/TPR): TP / (TP + FN).  Intuitively, the ability of the
  classifier to find all the positive samples.
+ Precision: TP / (TP + FP).  Intuitively, the ability
  of the classifier not to label as positive a sample that is negative.
+ F-beta score: Harmonic mean of precision and recall with $\beta$ chosen such
  that recall is considered $\beta$ times as important as precision,
  $$
  (1 + \beta^2) \frac{\text{precision} \cdot \text{recall}}
  {\beta^2 \text{precision} + \text{recall}}
  $$
  See [stackexchange
  post](https://stats.stackexchange.com/questions/221997/why-f-beta-score-define-beta-like-that)
  for the motivation of $\beta^2$.


When classification is obtained by dichotomizing a continuous score, the
receiver operating characteristic (ROC) curve gives a graphical summary of the
FPR and TPR for all thresholds. The ROC curve plots the TPR against the FPR at 
all thresholds.

+ Increasing from $(0, 0)$ to $(1, 1)$.
+ Best classification passes $(0, 1)$.
+ Classification by random guess gives the 45-degree line.
+ Area between the ROC and the 45-degree line is the Gini coefficient, a measure
  of inequality.
+ Area under the curve (AUC) of ROC thus provides an important metric of
classification results.


### Cross-validation

+ Goal: strike a bias-variance tradeoff.
+ K-fold: hold out each fold as testing data.
+ Scores: minimized to train a model

Cross-validation is an important measure to prevent over-fitting. Good in-sample
performance does not necessarily mean good out-sample performance. A general
work flow in model selection with cross-validation is as follows.

+ Split the data into training and testing
+ For each candidate model $m$ (with possibly multiple tuning parameters)
    - Fit the model to the training data
    - Obtain the performance measure $f(m)$ on the testing data (e.g., CV score,
      MSE, loss, etc.)
+ Choose the model $m^* = \arg\max_m f(m)$.


## Decision Trees

The goal of a decision tree algorithm is to create a model that predicts the value 
of a target variable by learning decision rules inferred from the features of the data. 
A decision tree can be viewed as a piecewise constant approximation of the target function.

### Algorithm Formulation

At any given node in the tree, the algorithm seeks the best split that minimizes an 
impurity or loss measure $H$. Let $Q_m$ represent the data at node $m$, with 
a sample size of $n_m$. A candidate split $\theta$ consists of a potential 
threshold for a chosen feature. After the split, the data are divided into two groups: 
$Q_{m,l}$ (left node) with sample size $n_{m,l}$, and $Q_{m,r}$ (right node) 
with sample size $n_{m,r}$. The quality of the split is measured by:

$$
G(Q_m, \theta) = \frac{n_{m,l}}{n_m} H(Q_{m,l}(\theta)) + \frac{n_{m,r}}{n_m} H(Q_{m,r}(\theta)).
$$

The algorithm selects the split that minimizes the impurity:

$$
\theta^* = \arg\min_{\theta} G(Q_m, \theta).
$$

This process is applied recursively at each child node, continuing until a stopping 
condition is met.

+ Stopping Criteria: 
  The algorithm stops when the maximum tree depth is reached or when the node sample 
  size falls below a preset threshold.
+ Pruning: Reduce the complexity of the final tree by removing branches that 
  add little predictive value. This reduces overfitting and improves the generalization 
  accuracy of the model.

### Matrics

See `sklearn` documentation for [details](https://scikit-learn.org/stable/modules/tree.html#classification-criteria).

#### Classification
In decision tree classification, several criteria can be used to
measure the quality of a split at each node. These criteria are based
on how "pure" the resulting nodes are after the split. A pure node
contains samples that predominantly belong to a single class. The goal
is to minimize impurity, leading to nodes that are as homogeneous as
possible.

+ Gini Index: The Gini index measures the impurity of a node by
  calculating the probability of randomly choosing two different
  classes. A perfect split (all instances belong to one class) has a
  Gini index of 0. At node $m$, the Gini index is
  $$
  H(Q_m) = \sum_{k=1}^{K} p_{mk} (1 - p_{mk}),
  $$
  where $p_{mk}$ is the proportion of samples of class $k$ at node $m$;
  and$K$ is the total number of classes
  The Gini index is often preferred for its speed and simplicity, and
  itâ€™s used by default in many implementations of decision trees,
  including `sklearn`.

+ Entropy (Information Gain): Entropy is another measure of impurity,
  derived from information theory. It quantifies the "disorder" of the
  data at a node. Lower entropy means higher purity. At node $m$, it
  is defined as
  $$
  H(Q_m) = - \sum_{k=1}^{K} p_{mk} \log p_{mk}
  $$
  Entropy is commonly used in decision tree algorithms like ID3 and
  C4.5. The choice between Gini and entropy often depends on specific
  use cases, but both perform similarly in practice.
  
+ Misclassification Error: Misclassification error focuses solely on
  the most frequent class in the node. It measures the proportion of
  samples that do not belong to the majority class. Although less
  sensitive than Gini and entropy, it can be useful for classification
  when simplicity is preferred. At node $m$, it is defined as
  $$
  H(Q_m) = 1 - \max_k p_{mk},
  $$
  where $\max_k p_{mk}$ is the largest proportion of samples belonging
  to any class $k$.

#### Regression Criteria

In decision tree regression, different criteria are used to assess the
quality of a split. The goal is to minimize the spread or variance of
the target variable within each node.

+ Mean Squared Error (MSE): Mean squared error is the most common
  criterion used in regression trees. It measures the average squared
  difference between the actual values and the predicted values (mean
  of the target in the node). The smaller the MSE, the better the
  fit. At node $m$, it is 
  $$
  H(Q_m) = \frac{1}{n_m} \sum_{i=1}^{n_m} (y_i - \bar{y}_m)^2,
  $$
  where
    - $y_i$ is the actual value for sample $i$;
    - $\bar{y}_m$ is the mean value of the target at node $m$;
    - $n_m$ is the number of samples at node $m$.

  MSE works well when the target is continuous and normally distributed.

+ Half Poisson Deviance (for count targets): When dealing with count
  data, the Poisson deviance is used to model the variance in the
  number of occurrences of an event. It is well-suited for target
  variables representing counts (e.g., number of occurrences of an
  event). At node $m$, it is
  $$
  H(Q_m) = \sum_{i=1}^{n_m} \left( y_i \log\left(\frac{y_i}{\hat{y}_i}\right) - (y_i - \hat{y}_i) \right),
  $$
  where $\hat{y}_i$ is the predicted count. This criterion is
  especially useful when the target variable represents discrete
  counts, such as predicting the number of occurrences of an event.

+ Mean Absolute Error (MAE): Mean absolute error is another criterion
  that minimizes the absolute differences between actual and predicted
  values. While it is more robust to outliers than MSE, it is slower
  computationally due to the lack of a closed-form solution for
  minimization. At node $m$, it is
  $$
  H(Q_m) = \frac{1}{n_m} \sum_{i=1}^{n_m} |y_i - \bar{y}_m|
  $$
  MAE is useful when you want to minimize large deviations and can be
  more robust in cases where outliers are present in the data.


#### Summary
In decision trees, the choice of splitting criterion depends on the
type of task (classification or regression) and the nature of the
data. For classification tasks, the Gini index and entropy are the
most commonly used, with Gini offering simplicity and speed, and
entropy providing a more theoretically grounded
approach. Misclassification error can be used for simpler cases. For
regression tasks, MSE is the most popular choice, but Poisson deviance
and MAE are useful for specific use cases such as count data and
robust models, respectively.


